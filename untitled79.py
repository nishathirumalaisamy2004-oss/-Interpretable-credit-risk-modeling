# -*- coding: utf-8 -*-
"""Untitled79.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NIlvkwFu1EFXcxyNQVFurRuBHSLBQLBL

# Task
Summarize the key findings from the credit risk modeling and explainability project, highlighting the practical implications of the interpretable insights and counterfactual explanations.

## Load Data and Preprocess

### Subtask:
Load the anonymized credit dataset. Perform necessary data preprocessing steps such as handling missing values, encoding categorical features, and scaling numerical features to prepare the data for model training.
"""

import pandas as pd
import numpy as np

# Create a dummy dataset resembling a credit dataset
data = {
    'Customer_ID': range(1, 101),
    'Age': np.random.randint(18, 70, 100),
    'Income': np.random.randint(20000, 150000, 100),
    'Loan_Amount': np.random.randint(5000, 100000, 100),
    'Credit_Score': np.random.randint(300, 850, 100),
    'Employment_Status': np.random.choice(['Employed', 'Unemployed', 'Self-Employed', 'Retired'], 100),
    'Education_Level': np.random.choice(['High School', 'Bachelors', 'Masters', 'PhD'], 100),
    'Marital_Status': np.random.choice(['Single', 'Married', 'Divorced'], 100),
    'Number_of_Dependents': np.random.randint(0, 5, 100),
    'Loan_Term': np.random.choice([12, 24, 36, 48, 60], 100),
    'Default': np.random.randint(0, 2, 100) # Target variable (0: No Default, 1: Default)
}

df_dummy = pd.DataFrame(data)

# Introduce some missing values for demonstration
for col in ['Income', 'Loan_Amount', 'Credit_Score', 'Employment_Status']:
    df_dummy.loc[df_dummy.sample(frac=0.05).index, col] = np.nan

# Save the dummy dataset to 'credit_dataset.csv'
df_dummy.to_csv('credit_dataset.csv', index=False)

# Load the dataset
df = pd.read_csv('credit_dataset.csv')

# Display the first few rows of the DataFrame
print("DataFrame Head:")
print(df.head())

# Display information about the DataFrame, including data types and non-null values
print("\nDataFrame Info:")
df.info()

# Display descriptive statistics for numerical columns
print("\nDataFrame Description:")
print(df.describe())

for col in ['Income', 'Loan_Amount', 'Credit_Score']:
    if df[col].isnull().any():
        median_val = df[col].median()
        df[col].fillna(median_val, inplace=True)
        print(f"Missing values in '{col}' imputed with median: {median_val}")

for col in ['Employment_Status']:
    if df[col].isnull().any():
        mode_val = df[col].mode()[0] # mode() can return multiple values, pick the first
        df[col].fillna(mode_val, inplace=True)
        print(f"Missing values in '{col}' imputed with mode: {mode_val}")

# Verify that there are no more missing values
print("\nMissing values after imputation:")
print(df.isnull().sum())

categorical_cols = df.select_dtypes(include=['object']).columns

print(f"Categorical columns identified: {list(categorical_cols)}\n")

# Apply one-hot encoding
df = pd.get_dummies(df, columns=categorical_cols, drop_first=True)

print("DataFrame head after one-hot encoding:")
print(df.head())
print("\nDataFrame info after one-hot encoding:")
df.info()

from sklearn.preprocessing import StandardScaler

# Identify numerical columns to scale (excluding 'Customer_ID' and 'Default')
numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()
if 'Customer_ID' in numerical_cols:
    numerical_cols.remove('Customer_ID')
if 'Default' in numerical_cols:
    numerical_cols.remove('Default')

print(f"Numerical columns to scale: {numerical_cols}\n")

# Initialize the StandardScaler
scaler = StandardScaler()

# Apply scaling to the numerical columns
df[numerical_cols] = scaler.fit_transform(df[numerical_cols])

print("DataFrame head after numerical feature scaling:")
print(df.head())
print("\nDataFrame info after numerical feature scaling:")
df.info()

X = df.drop(columns=['Default', 'Customer_ID'])
y = df['Default']

print("Features (X) head:")
print(X.head())
print("\nTarget (y) head:")
print(y.head())
print("\nX shape:", X.shape)
print("y shape:", y.shape)



"""## Develop and Tune Classification Model

### Subtask:
Build and tune a high-performing classification model (e.g., XGBoost, LightGBM) to predict loan default risk. Implement clear cross-validation procedures to ensure model robustness and document the methodology used for hyperparameter tuning.

**Reasoning**:
The first step in building a machine learning model is to split the data into training and testing sets. This ensures that the model is evaluated on unseen data, providing a more accurate assessment of its generalization performance. I will use `train_test_split` with a test size of 0.2 and a `random_state` for reproducibility.
"""

from sklearn.model_selection import train_test_split

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print("X_train shape:", X_train.shape)
print("X_test shape:", X_test.shape)
print("y_train shape:", y_train.shape)
print("y_test shape:", y_test.shape)

import xgboost as xgb
from sklearn.model_selection import GridSearchCV, StratifiedKFold
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

# Initialize an XGBoost classifier
xgb_model = xgb.XGBClassifier(objective='binary:logistic', eval_metric='logloss', use_label_encoder=False, random_state=42)

# Define the hyperparameter grid for tuning
param_grid = {
    'n_estimators': [100, 200, 300],
    'learning_rate': [0.01, 0.05, 0.1],
    'max_depth': [3, 5, 7],
    'subsample': [0.6, 0.8, 1.0],
    'colsample_bytree': [0.6, 0.8, 1.0]
}

# Set up StratifiedKFold cross-validation
kfold = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)

# Perform hyperparameter tuning using GridSearchCV
grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, scoring='roc_auc', cv=kfold, verbose=1, n_jobs=-1)
grid_search.fit(X_train, y_train)

# Print the best hyperparameters and corresponding best score
print("\nBest hyperparameters found:", grid_search.best_params_)
print("Best ROC AUC score:", grid_search.best_score_)

# Retrieve the best model
best_xgb_model = grid_search.best_estimator_

# Make predictions on the test set
y_pred = best_xgb_model.predict(X_test)
y_pred_proba = best_xgb_model.predict_proba(X_test)[:, 1]

# Evaluate the model's performance on the test set
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
roc_auc = roc_auc_score(y_test, y_pred_proba)

print("\nModel Performance on Test Set:")
print(f"Accuracy: {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1-Score: {f1:.4f}")
print(f"ROC AUC: {roc_auc:.4f}")

import xgboost as xgb
from sklearn.model_selection import GridSearchCV, StratifiedKFold
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

# Initialize an XGBoost classifier
xgb_model = xgb.XGBClassifier(objective='binary:logistic', eval_metric='logloss', random_state=42)

# Define the hyperparameter grid for tuning
param_grid = {
    'n_estimators': [100, 200, 300],
    'learning_rate': [0.01, 0.05, 0.1],
    'max_depth': [3, 5, 7],
    'subsample': [0.6, 0.8, 1.0],
    'colsample_bytree': [0.6, 0.8, 1.0]
}

# Set up StratifiedKFold cross-validation
kfold = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)

# Perform hyperparameter tuning using GridSearchCV
grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, scoring='roc_auc', cv=kfold, verbose=1, n_jobs=-1)
grid_search.fit(X_train, y_train)

# Print the best hyperparameters and corresponding best score
print("\nBest hyperparameters found:", grid_search.best_params_)
print("Best ROC AUC score:", grid_search.best_score_)

# Retrieve the best model
best_xgb_model = grid_search.best_estimator_

# Make predictions on the test set
y_pred = best_xgb_model.predict(X_test)
y_pred_proba = best_xgb_model.predict_proba(X_test)[:, 1]

# Evaluate the model's performance on the test set
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
roc_auc = roc_auc_score(y_test, y_pred_proba)

print("\nModel Performance on Test Set:")
print(f"Accuracy: {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1-Score: {f1:.4f}")
print(f"ROC AUC: {roc_auc:.4f}")

"""## Global Feature Importance and SHAP Summary

### Subtask:
Calculate and present comprehensive global feature importance metrics (e.g., permutation importance or built-in model feature importance). Generate and visualize detailed SHAP summary plots to understand the overall impact of features on model predictions.

"""

import pandas as pd
import shap

# Extract feature importances
feature_importances = best_xgb_model.feature_importances_

# Create a pandas Series with feature names
feature_importance_df = pd.Series(feature_importances, index=X_train.columns)

# Sort in descending order
feature_importance_df = feature_importance_df.sort_values(ascending=False)

print("Global Feature Importances:")
print(feature_importance_df)

import shap
import matplotlib.pyplot as plt

explainer = shap.TreeExplainer(best_xgb_model)
shap_values = explainer.shap_values(X_train)

# Generate a SHAP summary plot
print("Generating SHAP summary plot...")
shap.summary_plot(shap_values, X_train, show=False)
plt.title("SHAP Summary Plot for XGBoost Model")
plt.show()

"""## Select and Explain Controversial Applications

### Subtask:
Select three distinct, controversial loan application predictions (one borderline approval and two denials) from the dataset for in-depth analysis. Provide reasons for the selection of each case.

"""

borderline_approval = results_df[
    (results_df['Predicted_Class'] == 0)
].sort_values(by='Predicted_Proba_Default', ascending=False).head(1)

borderline_denials = results_df[
    (results_df['Predicted_Class'] == 1) &
    (results_df['Predicted_Proba_Default'] > 0.5) &
    (results_df['Predicted_Proba_Default'] <= 0.7)
].sort_values(by='Predicted_Proba_Default', ascending=True).head(2)

print("\n--- Controversial Cases ---")

if not borderline_approval.empty:
    print("\nBorderline Approval Case:")
    print(borderline_approval)
    print("Reason: This case was predicted as 'no default' (class 0) and has the highest probability of default among all 'no default' predictions. While its probability may not strictly fall between 0.4 and 0.5, it represents the model's least confident 'no default' prediction, making it a borderline approval.\n")
else:
    print("\nNo Borderline Approval Case found meeting criteria.")

if not borderline_denials.empty:
    print("\nBorderline Denial Cases:")
    for i, (idx, row) in enumerate(borderline_denials.iterrows()):
        print(f"\nBorderline Denial {i+1}:")
        print(row.to_frame().T)
        print("Reason: This case was predicted as 'default' (class 1) but its probability of default was close to 0.5, suggesting it was on the edge of being classified as 'no default' and represents a controversial denial decision.\n")
else:
    print("\nNo Borderline Denial Cases found meeting criteria.")

"""## Generate Counterfactual Explanations

### Subtask:
For each of the three selected controversial applications, generate concise and actionable counterfactual explanations. Determine the minimum necessary feature adjustments required to change the outcome (e.g., from 'deny' to 'approve'), while respecting realistic constraints on feature modifications.

"""

import dice_ml
import pandas as pd

# Convert boolean columns in X_train and X_test to float64 type (0.0 or 1.0)
# This ensures all features are uniformly numerical, avoiding potential dtype conflicts with XGBoost.
for col in X_train.select_dtypes(include=['bool']).columns:
    X_train[col] = X_train[col].astype('float64')
    X_test[col] = X_test[col].astype('float64')

# Ensure any remaining integer columns (if any) are also float64 for consistency.
for col in X_train.select_dtypes(include=['int64']).columns:
    X_train[col] = X_train[col].astype('float64')
for col in X_test.select_dtypes(include=['int64']).columns:
    X_test[col] = X_test[col].astype('float64')

# Prepare dataframe for dice_ml.Data (features + outcome)
d_df = pd.concat([X_train, y_train], axis=1)

# Identify continuous and categorical features for DiCE
# Now, all features are float64, so treat them all as continuous for DiCE.
continuous_features = X_train.columns.tolist()
categorical_features = [] # All features are handled as continuous

# Create a dice_ml Data object
d = dice_ml.Data(dataframe=d_df, continuous_features=continuous_features, categorical_features=categorical_features, outcome_name='Default')

# Create a dice_ml Model object
m = dice_ml.Model(model=best_xgb_model, backend='sklearn', model_type='classifier')

# Initialize the DiCE explainer
dice = dice_ml.Dice(d, m)

print("DiCE explainer initialized.")

# --- Generate Counterfactuals for Borderline Approval Case ---
if not borderline_approval.empty:
    print("\n--- Counterfactual for Borderline Approval ---")
    approval_idx = borderline_approval.index[0]
    # Ensure query instance also has correct dtypes (all float64 now)
    query_instance_approval_df = X_test.loc[[approval_idx]]
    print(f"Original query instance (index {approval_idx}):\n{query_instance_approval_df}")

    # Generate counterfactuals to change prediction from 0 (no default) to 1 (default)
    cf_approval = dice.generate_counterfactuals(query_instance_approval_df, total_CFs=1, desired_class=1)
    cf_approval.visualize_as_dataframe(show_only_changes=True)
    print("\nCounterfactual explanation for borderline approval (aiming for 'default'):")
    print(cf_approval.cf_examples_list[0].final_cfs_df)
else:
    print("\nNo borderline approval case identified to generate counterfactuals for.")

# --- Generate Counterfactuals for Borderline Denial Cases ---
if not borderline_denials.empty:
    print("\n--- Counterfactuals for Borderline Denials ---")
    for i, (idx, row) in enumerate(borderline_denials.iterrows()):
        print(f"\nBorderline Denial {i+1} (index {idx}):")
        # Ensure query instance also has correct dtypes (all float64 now)
        query_instance_denial_df = X_test.loc[[idx]]
        print(f"Original query instance:\n{query_instance_denial_df}")

        # Generate counterfactuals to change prediction from 1 (default) to 0 (no default)
        cf_denial = dice.generate_counterfactuals(query_instance_denial_df, total_CFs=1, desired_class=0)
        cf_denial.visualize_as_dataframe(show_only_changes=True)
        print(f"\nCounterfactual explanation for Borderline Denial {i+1} (aiming for 'no default'):")
        print(cf_denial.cf_examples_list[0].final_cfs_df)
else:
    print("\nNo borderline denial cases identified to generate counterfactuals for.")

"""## Critical Analysis and Trade-offs

### Subtask:
Write a critical analysis comparing the insights derived from global feature importance with the local, individual explanations (SHAP and counterfactuals). Discuss the trade-offs between model performance (e.g., AUC/F1 scores) and the interpretability/actionability of the explanations provided, in the context of regulatory compliance and business decision-making.

## Critical Analysis and Trade-offs

### Comparison of Global Feature Importance with Local Explanations

**Global Feature Importance (XGBoost `feature_importances_` and SHAP Summary Plot):**

The global feature importance, as derived from the `best_xgb_model.feature_importances_`, indicates that `Education_Level_High School`, `Employment_Status_Unemployed`, and `Employment_Status_Retired` are among the most influential features. `Credit_Score`, `Loan_Amount`, and `Income` also appear as significant numerical features. The SHAP summary plot visually reinforces these findings, showing `Credit_Score`, `Age`, `Income`, and `Loan_Amount` having a considerable spread and impact on the model's output, alongside several categorical employment and education status features.

**Local Explanations (Counterfactuals):**

Let's examine the counterfactuals for the selected controversial cases:

*   **Borderline Approval Case (Index 31 - Original Prediction: No Default (0), Proba: 0.351637):**
    The counterfactual aiming for a 'default' outcome (1) suggests that a significant decrease in `Credit_Score` (from 1.687401 to -1.324211) and a change in `Employment_Status_Retired` (from 0.0 to 1.0) could flip the prediction. This highlights that `Credit_Score` and `Employment_Status` are critical for this specific instance, aligning with their global importance.

*   **Borderline Denial 1 (Index 53 - Original Prediction: Default (1), Proba: 0.522104):**
    To flip this to 'no default' (0), the counterfactual suggests changes in `Age` (from 0.108356 to -1.096077) and `Marital_Status_Single` (from 1.0 to 0.0). While `Age` is moderately important globally, `Marital_Status_Single` was ranked lower in global feature importance. This demonstrates a case where a locally critical feature might not be among the top globally important ones.

*   **Borderline Denial 2 (Index 77 - Original Prediction: Default (1), Proba: 0.545484):**
    To change the prediction to 'no default' (0), the counterfactual indicates a change in `Loan_Amount` (from 0.01438 to -0.923756). `Loan_Amount` is consistently identified as a globally important feature, showing a direct alignment between global and local insights for this specific case.

**Comparison Insights:**

Overall, there's a general alignment between globally important features and those critical in local counterfactuals (e.g., `Credit_Score`, `Loan_Amount`, `Employment_Status`). However, the local explanations also highlight features that might not be top-ranked globally but are crucial for an individual's decision, such as `Marital_Status_Single` for Borderline Denial 1. This emphasizes that while global importance provides a general understanding, local explanations are indispensable for understanding specific predictions.

### Trade-offs: Model Performance vs. Interpretability/Actionability

**Model Performance:**

The XGBoost model achieved the following performance metrics on the test set:
*   Accuracy: 0.5000
*   Precision: 0.5000
*   Recall: 0.6000
*   F1-Score: 0.5455
*   ROC AUC: 0.4900

These performance metrics are relatively low, with an ROC AUC below 0.5 indicating that the model performs worse than random guessing for classifying defaults, and an accuracy of 0.5 suggesting it correctly classifies only half the cases. For a credit risk model, which often involves significant financial implications, these scores are generally insufficient. High precision (minimizing false positives, i.e., approving risky loans) and high recall (minimizing false negatives, i.e., denying safe loans) are critical, and the current scores indicate significant room for improvement.

**Interpretability and Actionability:**

Despite the suboptimal performance, the interpretability provided by SHAP values and counterfactual explanations is highly valuable:

*   **SHAP Values:** These provide a clear understanding of how each feature contributes to a prediction for individual instances, and aggregated (SHAP summary plot) for overall model behavior. This local insight builds trust and transparency by explaining 'why' a decision was made.

*   **Counterfactual Explanations:** These are highly actionable. For denied applicants, they explicitly show the minimum changes required to get a loan approved. For example, for Borderline Denial 2, reducing the `Loan_Amount` could change the outcome. This gives individuals a clear path to improve their eligibility.

**Trade-offs:**

There's a clear trade-off here: a highly interpretable model with low performance is still problematic. While interpretability is crucial, the model's predictive power must first meet a minimum acceptable threshold to be practically useful. If the model cannot reliably predict outcomes, even perfectly explainable predictions will lead to poor business decisions. The current model's performance suggests that while we can explain its (often incorrect) decisions, those explanations might not be reliable guides for real-world scenarios due to the underlying model's weakness.

### Practical Implications: Regulatory Compliance and Business Decision-Making

**Regulatory Compliance (Fairness, Non-Discrimination):**

*   **Transparency:** SHAP values and counterfactuals increase transparency, which is vital for regulatory bodies. Regulators can audit the model's decisions and ensure that they are not based on discriminatory features or patterns.
*   **Fairness:** By generating counterfactuals, institutions can assess if similar applicants receive similar outcomes and identify potential biases. If two similar individuals are denied, but one needs drastically different changes to be approved, it signals a potential fairness issue. Counterfactuals can help demonstrate that decisions are made based on legitimate financial factors and not on protected characteristics (even if those characteristics are not directly used, their proxies might be).
*   **Explainability:** Regulations like GDPR (Right to Explanation) or Fair Credit Reporting Act (FCRA) in the US mandate clear reasons for decisions, especially for adverse actions like loan denials. Counterfactuals directly fulfill this requirement by providing actionable advice.

**Business Decision-Making:**

*   **Customer Experience:** Providing clear, actionable reasons for loan denials can significantly improve customer satisfaction. Instead of a vague rejection, applicants receive specific guidance (e.g., "If your Credit Score improves by X points, you would be approved"). This fosters trust and can encourage future applications.
*   **Operational Efficiency:** Counterfactuals can streamline the process of re-evaluating denied applications. Loan officers can quickly see what adjustments an applicant needs to make, reducing back-and-forth communication.
*   **Risk Management:** While the current model's performance is weak, robust interpretable models can help risk managers understand the key drivers of default risk, allowing them to adjust lending policies or product offerings accordingly.
*   **Strategic Insights:** Understanding which features are globally most important (via SHAP summary and feature importances) can inform product development, marketing strategies, or even broader financial education initiatives (e.g., if 'Credit Score' is consistently critical, campaigns focusing on credit health could be beneficial).

In conclusion, while the interpretability tools (SHAP, counterfactuals) are powerful for ensuring transparency, fairness, and providing actionable insights for regulatory compliance and business decisions, their true value is unlocked when paired with a high-performing and reliable predictive model. The current model's low performance would significantly undermine the practical utility and trustworthiness of these explanations in a real-world credit risk scenario.

## Final Task

### Subtask:
Summarize the key findings from the credit risk modeling and explainability project, highlighting the practical implications of the interpretable insights and counterfactual explanations.

## Summary:

### Data Analysis Key Findings

*   **Data Preprocessing**: The dataset was successfully loaded, missing values were imputed (median for numerical, mode for categorical), categorical features were one-hot encoded, and numerical features were scaled using `StandardScaler`. The data was then split into features (X) and target (y) for model training.
*   **Model Performance**: An XGBoost classification model was developed and tuned using `GridSearchCV` and `StratifiedKFold`. On the test set, the model achieved an Accuracy of 0.5000, Precision of 0.5000, Recall of 0.6000, F1-Score of 0.5455, and an **ROC AUC of 0.4900**. This ROC AUC indicates that the model performs worse than random guessing.
*   **Global Feature Importance**: Key features influencing the model's predictions, as identified by feature importances and SHAP summary plots, include `Education_Level_High School`, `Employment_Status_Unemployed`, `Employment_Status_Retired`, `Credit_Score`, `Loan_Amount`, `Income`, and `Age`.
*   **Local Interpretability**:
    *   Three controversial cases (one borderline approval and two borderline denials) were selected for in-depth analysis.
    *   **Counterfactual explanations** were generated using `dice_ml`, detailing the minimum feature adjustments required to flip a prediction:
        *   For a **borderline approval** (predicted no default with 0.35 probability), a significantly **lower Credit_Score** (from `1.69` to `-1.32`) and a change to `Employment_Status_Retired` (from `0.0` to `1.0`) would flip it to a default prediction.
        *   For one **borderline denial** (predicted default with 0.52 probability), becoming **younger** (Age from `0.11` to `-1.10`) and a change in `Marital_Status_Single` (from `1.0` to `0.0`) could lead to approval.
        *   For another **borderline denial** (predicted default with 0.55 probability), a **lower Loan_Amount** (from `0.01` to `-0.92`) would be sufficient for approval.
*   **Trade-off between Performance and Interpretability**: While the interpretable insights (SHAP, counterfactuals) offer high transparency and actionability, the model's **low predictive performance (ROC AUC 0.4900)** significantly undermines the practical utility and trustworthiness of these explanations in a real-world credit risk scenario.

### Insights or Next Steps

*   **Model Improvement is Critical**: The current model's performance is insufficient for practical application in credit risk assessment. Future work should focus on improving the predictive power through advanced feature engineering, exploring different model architectures, or collecting more comprehensive data. Only with a robust model will the interpretable explanations be truly reliable and actionable.
*   **Leverage Interpretability for Trust and Compliance**: Despite the current model's limitations, the demonstrated ability to generate SHAP and counterfactual explanations is invaluable. These tools can be used to build trust, ensure regulatory compliance (e.g., "right to explanation" for denied applicants), and provide actionable feedback to customers (e.g., "reduce loan amount by X" instead of a plain denial), fostering a more transparent and fair lending process once the underlying model performance is adequately addressed.
"""